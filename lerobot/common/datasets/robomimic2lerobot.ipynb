{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ns1254/miniforge3/envs/lerobot/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from datasets import Dataset, Features, Image, Sequence, Value\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "from lerobot.common.datasets.lerobot_dataset import CODEBASE_VERSION\n",
    "from lerobot.common.datasets.push_dataset_to_hub.utils import (\n",
    "    concatenate_episodes,\n",
    "    get_default_encoding,\n",
    "    save_images_concurrently,\n",
    ")\n",
    "from lerobot.common.datasets.utils import (\n",
    "    calculate_episode_data_index,\n",
    "    hf_transform_to_torch,\n",
    ")\n",
    "from lerobot.common.datasets.video_utils import VideoFrame, encode_video_frames\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdf5_path = \"/home/ns1254/data_robomimic/nn/lift_image_v141_20p_abs.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = h5py.File(hdf5_path, \"r\")\n",
    "# demos = list(f[\"data\"].keys())\n",
    "\n",
    "# lengths=[]\n",
    "# for demo_name in demos:\n",
    "#     demo=f['data'][demo_name]\n",
    "#     num_samples=demo.attrs['num_samples']\n",
    "#     lengths.append(num_samples)\n",
    "\n",
    "# lengths=np.array(lengths)\n",
    "\n",
    "# print('Number of demos: ', len(demos))\n",
    "# print('Max length: ', np.max(lengths))\n",
    "# print('Min length: ', np.min(lengths))\n",
    "# print('Mean length: ', np.mean(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo=f['data'][demos[0]]\n",
    "# actions=demo['actions']\n",
    "# actions=np.array(actions)\n",
    "# demo['obs'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_keys=['actions', 'dones', 'rewards', 'states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs_keys = [key for key in demo['obs'].keys()]\n",
    "# obs_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs_keys_images=[key for key in obs_keys if 'image' in key]\n",
    "# obs_keys_others=[key for key in obs_keys if 'image' not in key]\n",
    "# obs_keys_images, obs_keys_others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robomimic_to_hf_dataset(data_dict, video) -> Dataset:\n",
    "    features = {} \n",
    "\n",
    "    keys = [key for key in data_dict if \"observation.images.\" in key]\n",
    "    for key in keys:\n",
    "        if video:\n",
    "            features[key] = VideoFrame()\n",
    "        else:\n",
    "            features[key] = Image()\n",
    "            \n",
    "    for key in data_dict.keys():\n",
    "        if 'observation.' in key and 'images' not in key:\n",
    "            features[key] = Sequence(length=data_dict[key].shape[1], feature=Value(dtype=\"float32\", id=None))\n",
    "\n",
    "    features[\"action\"] = Sequence(length=data_dict[\"action\"].shape[1], feature=Value(dtype=\"float32\", id=None))\n",
    "    features[\"episode_index\"] = Value(dtype=\"int64\", id=None)\n",
    "    features[\"frame_index\"] = Value(dtype=\"int64\", id=None)\n",
    "    features[\"timestamp\"] = Value(dtype=\"float32\", id=None)\n",
    "    features[\"next.done\"] = Value(dtype=\"bool\", id=None)\n",
    "    features[\"index\"] = Value(dtype=\"int64\", id=None)\n",
    "    features[\"dones\"] = Value(dtype=\"bool\", id=None)\n",
    "    features[\"rewards\"] = Value(dtype=\"float32\", id=None)\n",
    "\n",
    "    hf_dataset = Dataset.from_dict(data_dict, features=Features(features))\n",
    "    hf_dataset.set_transform(hf_transform_to_torch)\n",
    "    return hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_robomimic_hdf5(hdf5_path, videos_dir, fps, video, episodes, encoding):\n",
    "    f = h5py.File(hdf5_path, \"r\")\n",
    "    demos = list(f[\"data\"].keys())\n",
    "\n",
    "    lengths=[]\n",
    "    for demo_name in demos:\n",
    "        demo=f['data'][demo_name]\n",
    "        num_samples=demo.attrs['num_samples']\n",
    "        lengths.append(num_samples)\n",
    "\n",
    "    lengths=np.array(lengths)\n",
    "\n",
    "    print('Number of demos: ', len(demos))\n",
    "    print('Max length: ', np.max(lengths))\n",
    "    print('Min length: ', np.min(lengths))\n",
    "    print('Mean length: ', np.mean(lengths))\n",
    "\n",
    "    demo=f['data'][demos[0]]\n",
    "    demo_keys=['actions', 'dones', 'rewards', 'states']\n",
    "    obs_keys = [key for key in demo['obs'].keys()]\n",
    "    obs_keys_images=[key for key in obs_keys if 'image' in key]\n",
    "    obs_keys_others=[key for key in obs_keys if 'image' not in key]\n",
    "\n",
    "    num_episodes=len(demos)\n",
    "     \n",
    "    ep_dicts = []\n",
    "    ep_ids = range(num_episodes)\n",
    "    for ep_idx in tqdm.tqdm(ep_ids):\n",
    "        demo_name=demos[ep_idx]\n",
    "        episode=f['data'][demo_name]\n",
    "        num_frames=episode.attrs['num_samples']  \n",
    "\n",
    "        ep_dict = {} \n",
    "\n",
    "        for key in demo_keys:\n",
    "            ep_dict[key] = torch.from_numpy(episode[key][:])\n",
    "\n",
    "        for key in obs_keys_others:\n",
    "            ep_dict[f\"observation.{key}\"] = torch.from_numpy(episode['obs'][key][:])\n",
    "\n",
    "        ep_dict['observation.state'] = ep_dict['states']\n",
    "        del ep_dict['states']\n",
    "\n",
    "        ep_dict['action']=ep_dict['actions']\n",
    "        del ep_dict['actions']\n",
    "\n",
    "        ep_dict['next.done']=ep_dict['dones']\n",
    "\n",
    "        for key in obs_keys_images:\n",
    "            ep_dict[f\"observation.images.{key}\"] = torch.from_numpy(episode['obs'][key][:])\n",
    "        \n",
    "        ep_dict[\"episode_index\"] = torch.tensor([ep_idx] * num_frames)\n",
    "        ep_dict[\"frame_index\"] = torch.arange(0, num_frames, 1)\n",
    "        ep_dict[\"timestamp\"] = torch.arange(0, num_frames, 1) / fps \n",
    "        # TODO(rcadene): add reward and success by computing them in sim\n",
    "\n",
    "        assert isinstance(ep_idx, int)\n",
    "        ep_dicts.append(ep_dict)\n",
    "    \n",
    "    data_dict = concatenate_episodes(ep_dicts) \n",
    "    total_frames = data_dict[\"frame_index\"].shape[0]\n",
    "    data_dict[\"index\"] = torch.arange(0, total_frames, 1) \n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_robomimic_hdf5_to_lerobot_format(\n",
    "    hdf5_path: Path,\n",
    "    videos_dir: Path,\n",
    "    fps: int | None = None,\n",
    "    video: bool = True,\n",
    "    episodes: list[int] | None = None,\n",
    "    encoding: dict | None = None,\n",
    "): \n",
    "    if fps is None:\n",
    "        fps = 50\n",
    " \n",
    "    data_dict = load_from_robomimic_hdf5(hdf5_path, videos_dir, fps, video, episodes, encoding)\n",
    "    hf_dataset = robomimic_to_hf_dataset(data_dict, video=video)\n",
    "    episode_data_index = calculate_episode_data_index(hf_dataset)\n",
    "    info = {\n",
    "        \"codebase_version\": CODEBASE_VERSION,\n",
    "        \"fps\": fps,\n",
    "        \"video\": video,\n",
    "    }  \n",
    "    return hf_dataset, episode_data_index, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of demos:  40\n",
      "Max length:  63\n",
      "Min length:  38\n",
      "Mean length:  49.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 295.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['dones', 'rewards', 'observation.object', 'observation.robot0_eef_pos', 'observation.robot0_eef_quat', 'observation.robot0_eef_vel_ang', 'observation.robot0_eef_vel_lin', 'observation.robot0_gripper_qpos', 'observation.robot0_gripper_qvel', 'observation.robot0_joint_pos', 'observation.robot0_joint_pos_cos', 'observation.robot0_joint_pos_sin', 'observation.robot0_joint_vel', 'observation.state', 'action', 'next.done', 'observation.images.agentview_image', 'observation.images.robot0_eye_in_hand_image', 'episode_index', 'frame_index', 'timestamp', 'index'],\n",
      "    num_rows: 1992\n",
      "})\n",
      "{'from': tensor([   0,   54,  103,  153,  216,  268,  313,  360,  414,  454,  503,  560,\n",
      "         608,  651,  692,  740,  789,  830,  883,  930,  977, 1021, 1074, 1112,\n",
      "        1159, 1201, 1249, 1297, 1344, 1395, 1446, 1498, 1553, 1610, 1669, 1729,\n",
      "        1779, 1833, 1887, 1942]), 'to': tensor([  54,  103,  153,  216,  268,  313,  360,  414,  454,  503,  560,  608,\n",
      "         651,  692,  740,  789,  830,  883,  930,  977, 1021, 1074, 1112, 1159,\n",
      "        1201, 1249, 1297, 1344, 1395, 1446, 1498, 1553, 1610, 1669, 1729, 1779,\n",
      "        1833, 1887, 1942, 1992])}\n",
      "{'codebase_version': 'v1.6', 'fps': 20, 'video': False}\n"
     ]
    }
   ],
   "source": [
    "hdf5_path = Path(\"/home/ns1254/data_robomimic/nn/lift_image_v141_20p_abs.hdf5\")\n",
    "hf_dataset, episode_data_index, info = from_robomimic_hdf5_to_lerobot_format(hdf5_path, None, 20, False)\n",
    "print(hf_dataset)\n",
    "print(episode_data_index)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=LeRobotDataset.from_preloaded(LeRobotDataset, hf_dataset=hf_dataset, episode_data_index=episode_data_index, info=info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeRobotDataset(\n",
       "  Repository ID: '<class 'lerobot.common.datasets.lerobot_dataset.LeRobotDataset'>',\n",
       "  Split: 'train',\n",
       "  Number of Samples: 1992,\n",
       "  Number of Episodes: 40,\n",
       "  Type: image (.png),\n",
       "  Recorded Frames per Second: 20,\n",
       "  Camera Keys: ['observation.images.agentview_image', 'observation.images.robot0_eye_in_hand_image'],\n",
       "  Video Frame Keys: N/A,\n",
       "  Transformations: None,\n",
       "  Codebase Version: v1.6,\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average number of frames per episode: 49.800\n",
      "frames per second used during data collection: dataset.fps=20\n",
      "keys to access images from cameras: dataset.camera_keys=['observation.images.agentview_image', 'observation.images.robot0_eye_in_hand_image']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\naverage number of frames per episode: {dataset.num_samples / dataset.num_episodes:.3f}\")\n",
    "print(f\"frames per second used during data collection: {dataset.fps=}\")\n",
    "print(f\"keys to access images from cameras: {dataset.camera_keys=}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.common.datasets.populate_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute mean, min, max: 100%|█████████▉| 248/249 [00:04<00:00, 54.58it/s]\n",
      "Compute std: 100%|█████████▉| 248/249 [00:04<00:00, 60.66it/s]\n"
     ]
    }
   ],
   "source": [
    "stats = compute_stats(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute mean, min, max: 100%|█████████▉| 248/249 [00:04<00:00, 60.42it/s]\n",
      "Compute std: 100%|█████████▉| 248/249 [00:04<00:00, 60.14it/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1992/1992 [00:00<00:00, 41791.57 examples/s]\n",
      "/home/ns1254/lerobot/lerobot/scripts/push_dataset_to_hub.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  episode_data_index = {key: torch.tensor(episode_data_index[key]) for key in episode_data_index}\n"
     ]
    }
   ],
   "source": [
    "lerobot_dataset=dataset\n",
    "hf_dataset = lerobot_dataset.hf_dataset\n",
    "info = lerobot_dataset.info\n",
    "stats = lerobot_dataset.stats\n",
    "if stats==None: \n",
    "    stats = compute_stats(lerobot_dataset)\n",
    "    lerobot_dataset.stats = stats\n",
    "\n",
    "episode_data_index = lerobot_dataset.episode_data_index\n",
    "# local_dir = lerobot_dataset.videos_dir.parent\n",
    "local_dir  = Path(\"/home/ns1254/lerobot/data/lift\")\n",
    "meta_data_dir = local_dir / \"meta_data\"\n",
    "\n",
    "hf_dataset = hf_dataset.with_format(None)  # to remove transforms that cant be saved\n",
    "hf_dataset.save_to_disk(str(local_dir / \"train\")) \n",
    "save_meta_data(info, stats, episode_data_index, meta_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lerobot dataset from disk\n",
    "\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "\n",
    "local_dir = Path(\"/home/ns1254/lerobot/data\") \n",
    "lerobot_dataset = LeRobotDataset(repo_id=\"lift\", root=local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeRobotDataset(\n",
       "  Repository ID: 'lift',\n",
       "  Split: 'train',\n",
       "  Number of Samples: 1992,\n",
       "  Number of Episodes: 40,\n",
       "  Type: image (.png),\n",
       "  Recorded Frames per Second: 20,\n",
       "  Camera Keys: ['observation.images.agentview_image', 'observation.images.robot0_eye_in_hand_image'],\n",
       "  Video Frame Keys: N/A,\n",
       "  Transformations: None,\n",
       "  Codebase Version: v1.6,\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lerobot_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': {'max': tensor([0.0519, 0.0535, 1.0179, 2.5630, 2.7912, 0.3027, 1.0000]),\n",
       "  'mean': tensor([-0.0154, -0.0043,  0.8834,  1.9752,  2.1007,  0.0924, -0.4006]),\n",
       "  'min': tensor([-0.1071, -0.0466,  0.7865, -2.1443, -2.7041, -0.2459, -1.0000]),\n",
       "  'std': tensor([0.0328, 0.0183, 0.0586, 0.6690, 0.8172, 0.0826, 0.9163])},\n",
       " 'dones': {'max': tensor([1.]),\n",
       "  'mean': tensor([0.1009]),\n",
       "  'min': tensor([0.]),\n",
       "  'std': tensor([0.3012])},\n",
       " 'episode_index': {'max': tensor([39.]),\n",
       "  'mean': tensor([19.7751]),\n",
       "  'min': tensor([0.]),\n",
       "  'std': tensor([11.8135])},\n",
       " 'frame_index': {'max': tensor([62.]),\n",
       "  'mean': tensor([24.7093]),\n",
       "  'min': tensor([0.]),\n",
       "  'std': tensor([14.8983])},\n",
       " 'index': {'max': tensor([1991.]),\n",
       "  'mean': tensor([995.4998]),\n",
       "  'min': tensor([0.]),\n",
       "  'std': tensor([575.0409])},\n",
       " 'next.done': {'max': tensor([1.]),\n",
       "  'mean': tensor([0.1009]),\n",
       "  'min': tensor([0.]),\n",
       "  'std': tensor([0.3012])},\n",
       " 'observation.images.agentview_image': {'max': tensor([[[1.]],\n",
       "  \n",
       "          [[1.]],\n",
       "  \n",
       "          [[1.]]]),\n",
       "  'mean': tensor([[[0.8547]],\n",
       "  \n",
       "          [[0.8481]],\n",
       "  \n",
       "          [[0.8330]]]),\n",
       "  'min': tensor([[[0.0235]],\n",
       "  \n",
       "          [[0.0235]],\n",
       "  \n",
       "          [[0.0235]]]),\n",
       "  'std': tensor([[[0.2264]],\n",
       "  \n",
       "          [[0.2314]],\n",
       "  \n",
       "          [[0.2267]]])},\n",
       " 'observation.images.robot0_eye_in_hand_image': {'max': tensor([[[1.0000]],\n",
       "  \n",
       "          [[0.9922]],\n",
       "  \n",
       "          [[0.9804]]]),\n",
       "  'mean': tensor([[[0.8395]],\n",
       "  \n",
       "          [[0.8220]],\n",
       "  \n",
       "          [[0.8103]]]),\n",
       "  'min': tensor([[[0.0510]],\n",
       "  \n",
       "          [[0.0235]],\n",
       "  \n",
       "          [[0.0275]]]),\n",
       "  'std': tensor([[[0.2743]],\n",
       "  \n",
       "          [[0.2930]],\n",
       "  \n",
       "          [[0.2835]]])},\n",
       " 'observation.object': {'max': tensor([0.0326, 0.0501, 0.8858, 0.0381, 0.0333, 0.9983, 0.9959, 0.0332, 0.0408,\n",
       "          0.2047]),\n",
       "  'mean': tensor([-1.7597e-04, -3.8175e-03,  8.2470e-01, -1.1814e-03, -1.5931e-03,\n",
       "           6.3532e-01,  7.8490e-02, -2.3781e-02, -1.0539e-03,  6.6580e-02]),\n",
       "  'min': tensor([-0.0390, -0.0353,  0.8159, -0.0428, -0.0596,  0.0092, -0.9999, -0.1429,\n",
       "          -0.0396, -0.0107]),\n",
       "  'std': tensor([0.0169, 0.0184, 0.0123, 0.0078, 0.0079, 0.3075, 0.7039, 0.0392, 0.0107,\n",
       "          0.0671])},\n",
       " 'observation.robot0_eef_pos': {'max': tensor([0.0448, 0.0527, 1.0283]),\n",
       "  'mean': tensor([-0.0240, -0.0049,  0.8913]),\n",
       "  'min': tensor([-0.1228, -0.0452,  0.8090]),\n",
       "  'std': tensor([0.0398, 0.0166, 0.0642])},\n",
       " 'observation.robot0_eef_quat': {'max': tensor([0.9998, 0.3311, 0.1503, 0.0452]),\n",
       "  'mean': tensor([0.9911, 0.0335, 0.0484, 0.0011]),\n",
       "  'min': tensor([ 0.9417, -0.2107, -0.0066, -0.0588]),\n",
       "  'std': tensor([0.0105, 0.1143, 0.0267, 0.0163])},\n",
       " 'observation.robot0_eef_vel_ang': {'max': tensor([0.1985, 0.4882, 0.6792]),\n",
       "  'mean': tensor([ 0.0052, -0.0077,  0.0483]),\n",
       "  'min': tensor([-0.1629, -0.2694, -0.8478]),\n",
       "  'std': tensor([0.0515, 0.1118, 0.1853])},\n",
       " 'observation.robot0_eef_vel_lin': {'max': tensor([0.2433, 0.1286, 0.2394]),\n",
       "  'mean': tensor([ 0.0376, -0.0012, -0.0494]),\n",
       "  'min': tensor([-0.1003, -0.2040, -0.2523]),\n",
       "  'std': tensor([0.0687, 0.0258, 0.1069])},\n",
       " 'observation.robot0_gripper_qpos': {'max': tensor([ 0.0401, -0.0190]),\n",
       "  'mean': tensor([ 0.0337, -0.0340]),\n",
       "  'min': tensor([ 0.0181, -0.0404]),\n",
       "  'std': tensor([0.0076, 0.0073])},\n",
       " 'observation.robot0_gripper_qvel': {'max': tensor([0.0862, 0.1540]),\n",
       "  'mean': tensor([ 0.0002, -0.0005]),\n",
       "  'min': tensor([-0.1554, -0.0860]),\n",
       "  'std': tensor([0.0315, 0.0307])},\n",
       " 'observation.robot0_joint_pos': {'max': tensor([ 0.2176,  0.9715,  0.2097, -1.9029,  0.1751,  3.2820,  1.2750]),\n",
       "  'mean': tensor([-0.0158,  0.6720,  0.0050, -2.2169, -0.0068,  2.9804,  0.7113]),\n",
       "  'min': tensor([-0.2743,  0.1637, -0.1377, -2.6541, -0.1585,  2.7606, -0.0071]),\n",
       "  'std': tensor([0.0696, 0.2373, 0.0565, 0.1934, 0.0517, 0.1130, 0.2777])},\n",
       " 'observation.robot0_joint_pos_cos': {'max': tensor([ 1.0000,  0.9866,  1.0000, -0.3260,  1.0000, -0.9283,  1.0000]),\n",
       "  'mean': tensor([ 0.9975,  0.7598,  0.9984, -0.5901,  0.9986, -0.9807,  0.7280]),\n",
       "  'min': tensor([ 0.9626,  0.5640,  0.9781, -0.8835,  0.9847, -1.0000,  0.2915]),\n",
       "  'std': tensor([0.0052, 0.1303, 0.0027, 0.1434, 0.0022, 0.0164, 0.1691])},\n",
       " 'observation.robot0_joint_pos_sin': {'max': tensor([ 0.2159,  0.8258,  0.2082, -0.4684,  0.1742,  0.3718,  0.9566]),\n",
       "  'mean': tensor([-0.0158,  0.6064,  0.0050, -0.7842, -0.0068,  0.1596,  0.6290]),\n",
       "  'min': tensor([-0.2709,  0.1630, -0.1373, -0.9454, -0.1579, -0.1400, -0.0071]),\n",
       "  'std': tensor([0.0693, 0.1949, 0.0564, 0.1272, 0.0516, 0.1114, 0.2140])},\n",
       " 'observation.robot0_joint_vel': {'max': tensor([0.2325, 1.0793, 0.2401, 1.0589, 0.3103, 0.4919, 0.5750]),\n",
       "  'mean': tensor([-1.6778e-02,  2.0635e-01,  1.3502e-02,  1.7042e-01,  4.5128e-06,\n",
       "           4.0136e-02, -5.4972e-02]),\n",
       "  'min': tensor([-0.2388, -0.8094, -0.4305, -0.7070, -0.4134, -0.9060, -0.8845]),\n",
       "  'std': tensor([0.0676, 0.3672, 0.0673, 0.3174, 0.0924, 0.2135, 0.2282])},\n",
       " 'observation.state': {'max': tensor([ 3.1000,  0.2176,  0.9715,  0.2097, -1.9029,  0.1751,  3.2820,  1.2750,\n",
       "           0.0401, -0.0190,  0.0326,  0.0501,  0.8858,  0.9959,  0.0381,  0.0333,\n",
       "           0.9983,  0.2325,  1.0793,  0.2401,  1.0589,  0.3103,  0.4919,  0.5750,\n",
       "           0.0862,  0.1540,  0.0524,  0.1423,  0.2382,  2.3275,  1.5020,  1.6192]),\n",
       "  'mean': tensor([ 1.2355e+00, -1.5837e-02,  6.7195e-01,  4.9932e-03, -2.2169e+00,\n",
       "          -6.7945e-03,  2.9804e+00,  7.1132e-01,  3.3742e-02, -3.3969e-02,\n",
       "          -1.7597e-04, -3.8175e-03,  8.2470e-01,  7.8490e-02, -1.1814e-03,\n",
       "          -1.5931e-03,  6.3532e-01, -1.6778e-02,  2.0635e-01,  1.3502e-02,\n",
       "           1.7042e-01,  4.5130e-06,  4.0136e-02, -5.4972e-02,  1.9604e-04,\n",
       "          -5.0766e-04, -2.3649e-03,  1.1459e-03,  2.0836e-02,  1.3497e-03,\n",
       "           1.1067e-03,  1.7741e-03]),\n",
       "  'min': tensor([ 0.0000, -0.2743,  0.1637, -0.1377, -2.6541, -0.1585,  2.7606, -0.0071,\n",
       "           0.0181, -0.0404, -0.0390, -0.0353,  0.8159, -0.9999, -0.0428, -0.0596,\n",
       "           0.0092, -0.2388, -0.8094, -0.4305, -0.7070, -0.4134, -0.9060, -0.8845,\n",
       "          -0.1554, -0.0860, -0.1156, -0.1392, -0.2790, -1.0072, -3.0977, -1.5151]),\n",
       "  'std': tensor([0.7449, 0.0696, 0.2373, 0.0565, 0.1934, 0.0517, 0.1130, 0.2777, 0.0076,\n",
       "          0.0073, 0.0169, 0.0184, 0.0123, 0.7039, 0.0078, 0.0079, 0.3075, 0.0676,\n",
       "          0.3672, 0.0673, 0.3174, 0.0924, 0.2135, 0.2282, 0.0315, 0.0307, 0.0134,\n",
       "          0.0143, 0.0777, 0.1036, 0.1426, 0.0931])},\n",
       " 'rewards': {'max': tensor([1.]),\n",
       "  'mean': tensor([0.1009]),\n",
       "  'min': tensor([0.]),\n",
       "  'std': tensor([0.3012])},\n",
       " 'timestamp': {'max': tensor([3.1000]),\n",
       "  'mean': tensor([1.2355]),\n",
       "  'min': tensor([0.]),\n",
       "  'std': tensor([0.7449])}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lerobot_dataset.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
