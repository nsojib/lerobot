{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ns1254/miniforge3/envs/lerobot/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script demonstrates the use of `LeRobotDataset` class for handling and processing robotic datasets from Hugging Face.\n",
    "It illustrates how to load datasets, manipulate them, and apply transformations suitable for machine learning tasks in PyTorch.\n",
    "\n",
    "Features included in this script:\n",
    "- Loading a dataset and accessing its properties.\n",
    "- Filtering data by episode number.\n",
    "- Converting tensor data for visualization.\n",
    "- Saving video files from dataset frames.\n",
    "- Using advanced dataset features like timestamp-based frame selection.\n",
    "- Demonstrating compatibility with PyTorch DataLoader for batch processing.\n",
    "\n",
    "The script ends with examples of how to batch process data using PyTorch's DataLoader.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import imageio\n",
    "import torch\n",
    "\n",
    "import lerobot\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available datasets:\n"
     ]
    }
   ],
   "source": [
    "print(\"List of available datasets:\")\n",
    "# pprint(lerobot.available_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 212 files: 100%|██████████| 212/212 [00:00<00:00, 9893.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Let's take one for this example\n",
    "repo_id = \"lerobot/pusht\"\n",
    "\n",
    "# You can easily load a dataset from a Hugging Face repository\n",
    "dataset = LeRobotDataset(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeRobotDataset(\n",
       "  Repository ID: 'lerobot/pusht',\n",
       "  Split: 'train',\n",
       "  Number of Samples: 25650,\n",
       "  Number of Episodes: 206,\n",
       "  Type: video (.mp4),\n",
       "  Recorded Frames per Second: 10,\n",
       "  Camera Keys: ['observation.image'],\n",
       "  Video Frame Keys: ['observation.image'],\n",
       "  Transformations: None,\n",
       "  Codebase Version: v1.6,\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_is_protocol',\n",
       " 'camera_keys',\n",
       " 'delta_timestamps',\n",
       " 'episode_data_index',\n",
       " 'features',\n",
       " 'fps',\n",
       " 'from_preloaded',\n",
       " 'hf_dataset',\n",
       " 'image_transforms',\n",
       " 'info',\n",
       " 'num_episodes',\n",
       " 'num_samples',\n",
       " 'repo_id',\n",
       " 'root',\n",
       " 'split',\n",
       " 'stats',\n",
       " 'tolerance_s',\n",
       " 'video',\n",
       " 'video_backend',\n",
       " 'video_frame_keys',\n",
       " 'videos_dir']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 206 episodes from the dataset.\n"
     ]
    }
   ],
   "source": [
    "episodes = []\n",
    "for episode_index in range(dataset.num_episodes):\n",
    "    from_idx = dataset.episode_data_index[\"from\"][episode_index].item()\n",
    "    to_idx = dataset.episode_data_index[\"to\"][episode_index].item()\n",
    "    episode_frames = [dataset[idx][\"observation.image\"] for idx in range(from_idx, to_idx)]\n",
    "    episodes.append(episode_frames)\n",
    "\n",
    "print(f\"Extracted {len(episodes)} episodes from the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode=episodes[0]\n",
    "len(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 161)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_index=0\n",
    "from_idx = dataset.episode_data_index[\"from\"][episode_index].item()\n",
    "to_idx = dataset.episode_data_index[\"to\"][episode_index].item()\n",
    "\n",
    "from_idx, to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161, 279)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_index=1\n",
    "from_idx = dataset.episode_data_index[\"from\"][episode_index].item()\n",
    "to_idx = dataset.episode_data_index[\"to\"][episode_index].item()\n",
    "\n",
    "from_idx, to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['observation.image', 'observation.state', 'action', 'episode_index', 'frame_index', 'timestamp', 'next.reward', 'next.done', 'next.success', 'index'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row=dataset[10]\n",
    "row.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor(10), tensor(1.), tensor(10))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['episode_index'], row['frame_index'], row['timestamp'], row['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys \n",
    "# sys.path.append('/home/ns1254/lerobot/lerobot/common/datasets/push_dataset_to_hub/')\n",
    "\n",
    "# from aloha_hdf5_format import from_raw_to_lerobot_format\n",
    "\n",
    "# raw_dir = Path(\"/home/ns1254/act/dataset\")\n",
    "# hf_dataset, episode_data_index, info = from_raw_to_lerobot_format(raw_dir, None, 20, False)\n",
    "# dataset=LeRobotDataset.from_preloaded(LeRobotDataset, hf_dataset=hf_dataset, episode_data_index=episode_data_index, info=info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeRobotDataset(\n",
      "  Repository ID: 'lerobot/pusht',\n",
      "  Split: 'train',\n",
      "  Number of Samples: 25650,\n",
      "  Number of Episodes: 206,\n",
      "  Type: video (.mp4),\n",
      "  Recorded Frames per Second: 10,\n",
      "  Camera Keys: ['observation.image'],\n",
      "  Video Frame Keys: ['observation.image'],\n",
      "  Transformations: None,\n",
      "  Codebase Version: v1.6,\n",
      ")\n",
      "Dataset({\n",
      "    features: ['observation.image', 'observation.state', 'action', 'episode_index', 'frame_index', 'timestamp', 'next.reward', 'next.done', 'next.success', 'index'],\n",
      "    num_rows: 25650\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# LeRobotDataset is actually a thin wrapper around an underlying Hugging Face dataset\n",
    "# (see https://huggingface.co/docs/datasets/index for more information).\n",
    "print(dataset)\n",
    "print(dataset.hf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average number of frames per episode: 124.515\n",
      "frames per second used during data collection: dataset.fps=10\n",
      "keys to access images from cameras: dataset.camera_keys=['observation.image']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# And provides additional utilities for robotics and compatibility with Pytorch\n",
    "print(f\"\\naverage number of frames per episode: {dataset.num_samples / dataset.num_episodes:.3f}\")\n",
    "print(f\"frames per second used during data collection: {dataset.fps=}\")\n",
    "print(f\"keys to access images from cameras: {dataset.camera_keys=}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access frame indexes associated to first episode\n",
    "episode_index = 0\n",
    "from_idx = dataset.episode_data_index[\"from\"][episode_index].item()\n",
    "to_idx = dataset.episode_data_index[\"to\"][episode_index].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LeRobot datasets actually subclass PyTorch datasets so you can do everything you know and love from working\n",
    "# with the latter, like iterating through the dataset. Here we grab all the image frames.\n",
    "frames = [dataset[idx][\"observation.image\"] for idx in range(from_idx, to_idx)]\n",
    "\n",
    "# Video frames are now float32 in range [0,1] channel first (c,h,w) to follow pytorch convention. To visualize\n",
    "# them, we convert to uint8 in range [0,255]\n",
    "frames = [(frame * 255).type(torch.uint8) for frame in frames]\n",
    "# and to channel last (h,w,c).\n",
    "frames = [frame.permute((1, 2, 0)).numpy() for frame in frames]\n",
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Finally, we save the frames to a mp4 video for visualization.\n",
    "# Path(\"outputs/examples/1_load_lerobot_dataset\").mkdir(parents=True, exist_ok=True)\n",
    "# imageio.mimsave(\"outputs/examples/1_load_lerobot_dataset/episode_0.mp4\", frames, fps=dataset.fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 212 files: 100%|██████████| 212/212 [00:00<00:00, 18430.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dataset[0]['observation.image'].shape=torch.Size([4, 3, 96, 96])\n",
      "dataset[0]['observation.state'].shape=torch.Size([8, 2])\n",
      "dataset[0]['action'].shape=torch.Size([64, 2])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# For many machine learning applications we need to load the history of past observations or trajectories of\n",
    "# future actions. Our datasets can load previous and future frames for each key/modality, using timestamps\n",
    "# differences with the current loaded frame. For instance:\n",
    "delta_timestamps = {\n",
    "    # loads 4 images: 1 second before current frame, 500 ms before, 200 ms before, and current frame\n",
    "    \"observation.image\": [-1, -0.5, -0.20, 0],\n",
    "    # loads 8 state vectors: 1.5 seconds before, 1 second before, ... 20 ms, 10 ms, and current frame\n",
    "    \"observation.state\": [-1.5, -1, -0.5, -0.20, -0.10, -0.02, -0.01, 0],\n",
    "    # loads 64 action vectors: current frame, 1 frame in the future, 2 frames, ... 63 frames in the future\n",
    "    \"action\": [t / dataset.fps for t in range(64)],\n",
    "}\n",
    "dataset = LeRobotDataset(repo_id, delta_timestamps=delta_timestamps)\n",
    "print(f\"\\n{dataset[0]['observation.image'].shape=}\")  # (4,c,h,w)\n",
    "print(f\"{dataset[0]['observation.state'].shape=}\")  # (8,c)\n",
    "print(f\"{dataset[0]['action'].shape=}\\n\")  # (64,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, our datasets are fully compatible with PyTorch dataloaders and samplers because they are just\n",
    "# PyTorch datasets.\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=0,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch['observation.image'].shape=torch.Size([32, 3, 96, 96])\n",
      "batch['observation.state'].shape=torch.Size([32, 2])\n",
      "batch['action'].shape=torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(f\"{batch['observation.image'].shape=}\")  # (32,4,c,h,w)\n",
    "    print(f\"{batch['observation.state'].shape=}\")  # (32,8,c)\n",
    "    print(f\"{batch['action'].shape=}\")  # (32,64,c)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['observation.image', 'observation.state', 'action', 'episode_index', 'frame_index', 'timestamp', 'next.reward', 'next.done', 'next.success', 'index', 'observation.image_is_pad', 'observation.state_is_pad', 'action_is_pad'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
