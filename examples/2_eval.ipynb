{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ns1254/miniforge3/envs/lerobot/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import gym_pusht  # noqa: F401\n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "import numpy\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "import einops\n",
    "\n",
    "from lerobot.common.policies.diffusion.modeling_diffusion import DiffusionPolicy\n",
    "from lerobot.common.policies.utils import (\n",
    "    get_device_from_parameters,\n",
    "    get_dtype_from_parameters,\n",
    "    populate_queues,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the diffusion policy for pusht environment\n",
    "# pretrained_policy_path = Path(snapshot_download(\"lerobot/diffusion_pusht\"))\n",
    "# OR uncomment the following to evaluate a policy from the local outputs/train folder.\n",
    "pretrained_policy_path = Path(\"outputs/train/example_pusht_diffusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from local directory\n"
     ]
    }
   ],
   "source": [
    "policy = DiffusionPolicy.from_pretrained(pretrained_policy_path)\n",
    "policy.eval()\n",
    "policy.to(device)\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluation environment to render two observation types:\n",
    "# an image of the scene and state/position of the agent. The environment\n",
    "# also automatically stops running after 300 interactions/steps.\n",
    "env = gym.make(\n",
    "    \"gym_pusht/PushT-v0\",\n",
    "    obs_type=\"pixels_agent_pos\",\n",
    "    max_episode_steps=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the policy and environmens to prepare for rollout\n",
    "policy.reset()\n",
    "numpy_observation, info = env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixels (96, 96, 3)\n",
      "agent_pos (2,)\n"
     ]
    }
   ],
   "source": [
    "for key in numpy_observation.keys():\n",
    "    print(key, numpy_observation[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2]), torch.Size([1, 3, 96, 96]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.from_numpy(numpy_observation[\"agent_pos\"])\n",
    "image = torch.from_numpy(numpy_observation[\"pixels\"])\n",
    "\n",
    "# Convert to float32 with image from channel first in [0,255]\n",
    "# to channel last in [0,1]\n",
    "state = state.to(torch.float32)\n",
    "image = image.to(torch.float32) / 255\n",
    "image = image.permute(2, 0, 1)\n",
    "\n",
    "# Send data tensors from CPU to GPU\n",
    "state = state.to(device, non_blocking=True)\n",
    "image = image.to(device, non_blocking=True)\n",
    "\n",
    "# Add extra (empty) batch dimension, required to forward the policy\n",
    "state = state.unsqueeze(0)\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "# Create the policy input dictionary\n",
    "observation = {\n",
    "    \"observation.state\": state,\n",
    "    \"observation.image\": image,\n",
    "}\n",
    "\n",
    "observation['observation.state'].shape, observation['observation.image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the next action with respect to the current observation\n",
    "with torch.inference_mode():\n",
    "    action = policy.select_action(observation)\n",
    "\n",
    "action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([175.30476, 421.1422 ], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the action for the environment\n",
    "numpy_action = action.squeeze(0).to(\"cpu\").numpy()\n",
    "numpy_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Select a single action given environment observations.\n",
    "\n",
    "This method handles caching a history of observations and an action trajectory generated by the\n",
    "underlying diffusion model. Here's how it works:\n",
    "    - `n_obs_steps` steps worth of observations are cached (for the first steps, the observation is\n",
    "    copied `n_obs_steps` times to fill the cache).\n",
    "    - The diffusion model generates `horizon` steps worth of actions.\n",
    "    - `n_action_steps` worth of actions are actually kept for execution, starting from the current step.\n",
    "Schematically this looks like:\n",
    "    ----------------------------------------------------------------------------------------------\n",
    "    (legend: o = n_obs_steps, h = horizon, a = n_action_steps)\n",
    "    |timestep            | n-o+1 | n-o+2 | ..... | n     | ..... | n+a-1 | n+a   | ..... | n-o+h |\n",
    "    |observation is used | YES   | YES   | YES   | YES   | NO    | NO    | NO    | NO    | NO    |\n",
    "    |action is generated | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   |\n",
    "    |action is used      | NO    | NO    | NO    | YES   | YES   | YES   | NO    | NO    | NO    |\n",
    "    ----------------------------------------------------------------------------------------------\n",
    "Note that this means we require: `n_action_steps <= horizon - n_obs_steps + 1`. Also, note that\n",
    "\"horizon\" may not the best name to describe what the variable actually means, because this period is\n",
    "actually measured from the first observation which (if `n_obs_steps` > 1) happened in the past.\n",
    "\"\"\"\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = policy.normalize_inputs(batch)\n",
    "if len(policy.expected_image_keys) > 0:\n",
    "    batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original\n",
    "    batch[\"observation.images\"] = torch.stack([batch[k] for k in policy.expected_image_keys], dim=-4)\n",
    "# Note: It's important that this happens after stacking the images into a single key.\n",
    "policy._queues = populate_queues(policy._queues, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy._queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['observation.state', 'action', 'observation.images']), 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy._queues.keys(), len( policy._queues['observation.images'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(policy._queues[\"action\"]) == 0:\n",
    "    # stack n latest observations from the queue\n",
    "    batch = {k: torch.stack(list(policy._queues[k]), dim=1) for k in batch if k in policy._queues}\n",
    "    actions = policy.diffusion.generate_actions(batch)\n",
    "\n",
    "    # TODO(rcadene): make above methods return output dictionary?\n",
    "    actions = policy.unnormalize_outputs({\"action\": actions})[\"action\"]\n",
    "\n",
    "    policy._queues[\"action\"].extend(actions.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([tensor([[203.8272, 424.9017]], device='cuda:0'),\n",
       "       tensor([[226.5591, 426.3619]], device='cuda:0'),\n",
       "       tensor([[243.5239, 421.2686]], device='cuda:0'),\n",
       "       tensor([[259.5940, 418.5714]], device='cuda:0'),\n",
       "       tensor([[277.0004, 414.8833]], device='cuda:0'),\n",
       "       tensor([[293.9783, 412.5151]], device='cuda:0'),\n",
       "       tensor([[313.9379, 407.7516]], device='cuda:0')],\n",
       "      maxlen=8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy._queues[\"action\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[203.8272, 424.9017]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = policy._queues[\"action\"].popleft()\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation.state torch.Size([1, 2, 2])\n",
      "observation.images torch.Size([1, 2, 1, 3, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "batch = {k: torch.stack(list(policy._queues[k]), dim=1) for k in batch if k in policy._queues}\n",
    "for key in batch.keys():\n",
    "    print(key, batch[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = policy.diffusion.generate_actions(batch)\n",
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function expects `batch` to have:\n",
    "{\n",
    "    \"observation.state\": (B, n_obs_steps, state_dim)\n",
    "\n",
    "    \"observation.images\": (B, n_obs_steps, num_cameras, C, H, W)\n",
    "        AND/OR\n",
    "    \"observation.environment_state\": (B, environment_dim)\n",
    "}\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, n_obs_steps = batch[\"observation.state\"].shape[:2]\n",
    "assert n_obs_steps == policy.diffusion.config.n_obs_steps\n",
    "\n",
    "batch_size, n_obs_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 132])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode image features and concatenate them all together along with the state vector.\n",
    "global_cond = policy.diffusion._prepare_global_conditioning(batch)  # (B, global_cond_dim)\n",
    "global_cond.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.diffusion.config.use_separate_rgb_encoder_per_camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.diffusion._use_env_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 64]), torch.Size([1, 132]), torch.Size([1, 2, 2]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# global_cond = policy.diffusion._prepare_global_conditioning(batch)\n",
    "batch_size, n_obs_steps = batch[\"observation.state\"].shape[:2]\n",
    "global_cond_feats = [batch[\"observation.state\"]] \n",
    "\n",
    "# Combine batch, sequence, and \"which camera\" dims before passing to shared encoder.\n",
    "img_features = policy.diffusion.rgb_encoder( einops.rearrange(batch[\"observation.images\"], \"b s n ... -> (b s n) ...\"))\n",
    "# Separate batch dim and sequence dim back out. The camera index dim gets absorbed into the\n",
    "# feature dim (effectively concatenating the camera features).\n",
    "img_features = einops.rearrange( img_features, \"(b s n) ... -> b s (n ...)\", b=batch_size, s=n_obs_steps)\n",
    "global_cond_feats.append(img_features)\n",
    "\n",
    "# Concatenate features then flatten to (B, global_cond_dim).\n",
    "gc=torch.cat(global_cond_feats, dim=-1).flatten(start_dim=1)\n",
    "\n",
    "img_features.shape, gc.shape, batch[\"observation.state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiffusionRgbEncoder(\n",
       "  (center_crop): CenterCrop(size=[84, 84])\n",
       "  (maybe_random_crop): RandomCrop(size=(84, 84), padding=None)\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pool): SpatialSoftmax(\n",
       "    (nets): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (out): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.diffusion.rgb_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run sampling\n",
    "actions = policy.diffusion.conditional_sample(batch_size, global_cond=global_cond)\n",
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract `n_action_steps` steps worth of actions (from the current observation).\n",
    "start = n_obs_steps - 1\n",
    "end = start + policy.diffusion.config.n_action_steps\n",
    "actions = actions[:, start:end]\n",
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_obs_steps, policy.diffusion.config.n_action_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = policy.diffusion.conditional_sample(batch_size, global_cond=global_cond)\n",
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator =None\n",
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0), torch.float32, torch.Size([1, 16, 2]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_device_from_parameters(policy.diffusion)\n",
    "dtype = get_dtype_from_parameters(policy.diffusion)\n",
    "\n",
    "# Sample prior.\n",
    "sample = torch.randn(\n",
    "    size=(batch_size, policy.diffusion.config.horizon, policy.diffusion.config.output_shapes[\"action\"][0]),\n",
    "    dtype=dtype,\n",
    "    device=device,\n",
    "    generator=generator,\n",
    ")\n",
    "\n",
    "device, dtype, sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.diffusion.num_inference_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.diffusion.noise_scheduler.set_timesteps(policy.diffusion.num_inference_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDPMScheduler {\n",
       "  \"_class_name\": \"DDPMScheduler\",\n",
       "  \"_diffusers_version\": \"0.31.0\",\n",
       "  \"beta_end\": 0.02,\n",
       "  \"beta_schedule\": \"squaredcos_cap_v2\",\n",
       "  \"beta_start\": 0.0001,\n",
       "  \"clip_sample\": true,\n",
       "  \"clip_sample_range\": 1.0,\n",
       "  \"dynamic_thresholding_ratio\": 0.995,\n",
       "  \"num_train_timesteps\": 100,\n",
       "  \"prediction_type\": \"epsilon\",\n",
       "  \"rescale_betas_zero_snr\": false,\n",
       "  \"sample_max_value\": 1.0,\n",
       "  \"steps_offset\": 0,\n",
       "  \"thresholding\": false,\n",
       "  \"timestep_spacing\": \"leading\",\n",
       "  \"trained_betas\": null,\n",
       "  \"variance_type\": \"fixed_small\"\n",
       "}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.diffusion.noise_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in policy.diffusion.noise_scheduler.timesteps:\n",
    "    # Predict model output.\n",
    "    model_output = policy.diffusion.unet(\n",
    "        sample,\n",
    "        torch.full(sample.shape[:1], t, dtype=torch.long, device=sample.device),\n",
    "        global_cond=global_cond,\n",
    "    )\n",
    "    # Compute previous image: x_t -> x_t-1\n",
    "    sample = policy.diffusion.noise_scheduler.step(model_output, t, sample, generator=generator).prev_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1, 16, 2], [1, 132] -> [1, 16, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy.diffusion.unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
